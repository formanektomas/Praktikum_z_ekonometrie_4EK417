\documentclass[usenames,dvipsnames]{beamer}
%
% Choose how your presentation looks.
%
\usepackage[T1]{fontenc}
\usepackage[utf8]{inputenc}
\usepackage{lmodern}  
\usepackage{tikz}%boxy s vysvetlivkami 
\usetikzlibrary{calc}
\usepackage{amsmath}
\usepackage{bm}
\usepackage{color}
\usepackage{listings}

\newcommand{\mytikzmark}[2]{%
  \tikz[remember picture,inner sep=0pt,outer sep=0pt,baseline,anchor=base] 
    \node (#1) {\ensuremath{#2}};}
%    
\newcommand*\circled[1]{\tikz[baseline=(char.base)]{
    \node[shape=circle,draw=Red,inner sep=2pt] (char) {#1};}}
%
\newcommand*\circledd[1]{\tikz[baseline=(char.base)]{
    \node[shape=circle,draw=ProcessBlue, dashed, inner sep=2pt] (char) {#1};}}
%
% For more themes, color themes and font themes, see:
% http://deic.uab.es/~iblanes/beamer_gallery/index_by_theme.html
%
\mode<presentation>
{
  \usetheme{Darmstadt}      % or try Darmstadt, Madrid, Warsaw, ...
  \usecolortheme{default} % or try albatross, beaver, crane, ...
  \usefonttheme{serif}  % or try default, serif, structurebold, ...
  \setbeamertemplate{navigation symbols}{}
  \setbeamertemplate{caption}[numbered]
  \setbeamertemplate{headline}{}
} 
%
%
\title[IVR]{Praktikum z ekonometrie - Týden 9 \\IVR and 2SLS\\Repetition from previous courses} % The short title appears at the bottom of every slide, the full title is only on the title page
\author{VŠE Praha} % Your name
\institute[4EK417] % Your institution as it will appear on the bottom of every slide, may be shorthand to save space
{
% Your institution for the title page
\medskip
\textit{Tomáš Formánek} % Your email address
}
\date{} % Date, can be changed to a custom date

\begin{document}
 
\begin{frame}
  \titlepage
\end{frame}

% Uncomment these lines for an automatically generated outline.
%\begin{frame}{Outline}
%  \tableofcontents
%\end{frame}

%---------------------------------------------
\section{Introduction \& repetition from BSc courses}
\begin{frame}{Introduction: instrumental variables}
\textbf{Example}:  $\log(\textit{wage}_{\,i})=\beta_0+\beta_1 \textit{educ}_{\,i}+ [\textit{abil}_{\,i} + u_i]$  \\
\vspace{0.3cm}
\textbf{Instrumental variables}
\begin{enumerate}
\item Not in the main (structural) equation: no effect on the dependent variable after controlling for observed regressors.
\item Correlated (positively or negatively) with the endogenous regressor  (this can be tested).
\item Not correlated with the error term (in some cases, this can be tested, see Sargan test discussed next).
\end{enumerate}
\bigskip
\begin{itemize}
\item Possible IVs: father's education, mother's education, number of siblings, etc.\\ 
\smallskip
Usually, $\textit{IQ}$ is not a good IV - it's often correlated with $\textit{abil}$, i.e. with the error term $[\textit{abil}_{\,i} + u_i]$.
\end{itemize}
\end{frame}
%---------------------------------------------
\begin{frame}{Instrumental variables}
\begin{itemize}
    \item $y_i = \beta_0 + \beta_1 x_i + u_i$ \qquad SLRM with endogenous regressor $x$:
    $$
    \begin{matrix}
    y & \leftarrow & x \\
      & \nwarrow & | \\
      & & u
    \end{matrix}
    \qquad \qquad \textnormal{and} \qquad
    \frac{\textnormal{d} \, y}{\textnormal{d} \, x} = \beta_1 + \frac{\textnormal{d} \, u}{\textnormal{d} \, x}
    $$
    ~\\
    \item $y_i = \bm{x}_i \bm{\beta} + u_i$ \qquad MLRM with endogenous regressor(s):
    \begin{align*}
    \bm{\hat{\beta}} &= (\bm{X}^{\prime}\bm{X})^{-1} \bm{X}^{\prime}\bm{y} &|~ \textnormal{subs. for~} \bm{y} \\
    \bm{\hat{\beta}} &= (\bm{X}^{\prime}\bm{X})^{-1} \bm{X}^{\prime}(\bm{X \beta} + \bm{u}) &|~ \textnormal{rearr. \& take expects.} \\
    E[\bm{\hat{\beta}}] &= \bm{\beta} + E[(\bm{X}^{\prime}\bm{X})^{-1} \bm{X}^{\prime}\bm{u}] \neq \bm{\beta}
    \end{align*}
    \item With endogenous regressors, $E[(\bm{X}^{\prime}\bm{X})^{-1} \bm{X}^{\prime}\bm{u}] \neq \bm{0}$. \\Thus, OLS is biased (and asymptotically biased).
\end{itemize}
\end{frame}
%---------------------------------------------
\begin{frame}{Instrumental variables}
\begin{itemize}
    \item $y_i = \beta_0 + \beta_1 x_i + u_i$ \qquad IVR principle (SLRM):
    $$
    \begin{matrix}
    y & \leftarrow & x & \leftarrow & z \\
      & \nwarrow & | & &\\
      & & u & &
    \end{matrix}
    \qquad \qquad \textnormal{and} \qquad
    \frac{\textnormal{d} \, y}{\textnormal{d} \, x} = 
    \frac{\textnormal{d} \, y ~/~ \textnormal{d} \, z}
    {\textnormal{d} \, x ~/~ \textnormal{d} \, z}
    $$
    \item $y_i = \bm{x}_i \bm{\beta} + u_i$ \qquad IVR in MLRMs:
    \begin{align*}
    \bm{\hat{\beta}}_{\textnormal{OLS}} &= 
    (\bm{X}^{\prime}\bm{X})^{-1} \bm{X}^{\prime}\bm{y} \\
    \bm{\hat{\beta}}_{\textnormal{IV}} &= 
    (\bm{Z}^{\prime}\bm{X})^{-1} \bm{Z}^{\prime}\bm{y}
    \end{align*}
    where $\bm{Z}$ is a matrix of instruments, same dimensions as $\bm{X}$.
    \small{
    \begin{itemize}
        \item $\bm{Z}$ follows from $\bm{X}$, each endogenous regressor (column) is replaced by unique instrument (full column ranks of $\bm{X}$,$\bm{Z}$).
        \item Exact identification: \# endogenous regressors~ =~ \# IVs
        \item In IVR, $R^2$ has no interpretation (SST $\neq$ SSE + SSR).
        \item For IVR, we use specialized robust standard errors
        \item \textbf{IVR estimator is biased and consistent.}
    \end{itemize}
    }
\end{itemize}
\end{frame}
%---------------------------------------------
\begin{frame}{Instrumental variables: over-identification}
\vspace{-0.5cm}
$$y_{i1}=\beta_0 + \beta_1 y_{i2} + \beta_2 x_{i2} + \dots + \beta_k x_{ik} + u_{i} \quad|~z_1, z_2, z_3 \textnormal{~ are IVs for } y_2$$

\begin{itemize}
    \item By choosing any of the $z_1, z_2, z_3$ IVs \\(or any linear combination of), we perform IVR
    \item $\hat{\bm{\beta}}_{\textnormal{\,IV}}$ values change, as IV  in moment equations changes.
    \item We cannot ``simply'' use all three instruments. \\If \# columns in $\bm{Z}$ ($l$) ~>~ \# columns in $\bm{X}$ ($k$),\\
    $\bm{Z}^{\prime}\bm{X}$ is $(l\times k)$ with rank $k$ and no inverse:
    \\$\bm{\hat{\beta}}_{\textnormal{IV}} = (\bm{Z}^{\prime}\bm{X})^{-1} \bm{Z}^{\prime}\bm{y}$ cannot be calculated \\
    \medskip
    \item Solution: Project $\bm{X}$ to the space column of $\bm{Z}$ (GMM).\\
    ($\bm{X}$ has an endogenous column, $\bm{Z}$ is purely exogenous).
\end{itemize}
\end{frame}
%---------------------------------------------
\begin{frame}{Instrumental variables: over-identification}
\begin{block}{Projection matrices - repetition}
\vspace{-0.5cm}
\begin{align*} 
\hat{\bm{y}} & = \bm{X}\hat{\bm{\beta}} =  
\bm{X}(\bm{X}^{\prime}\bm{X})^{-1} \bm{X}^{\prime}\bm{y} = \bm{Py}\\
\bm{y} & = \hat{\bm{y}} + \hat{\bm{u}} = \bm{Py} + \bm{My}, \textnormal{~where} \\
\bm{M} & = \bm{I}  - \bm{X}(\bm{X}^{\prime}\bm{X})^{-1} \bm{X}^{\prime} = \bm{I} - \bm{P}
\end{align*}
\end{block}
\begin{itemize}
    \item Projection of columns of $\bm{X}$ in the column space of $\bm{Z}$:
$$
\hat{\bm{X}} = 
\bm{Z}(\bm{Z}^{\prime}\bm{Z})^{-1} \bm{Z}^{\prime}\bm{X},
$$
\item Columns of $\hat{\bm{X}}$ are linear combinations of columns in $\bm{Z}$, \\i.e. exogenous.
\medskip
\item IV estimator (over-identification):
\begin{align*}
 \bm{\hat{\beta}}_{\textnormal{IV}} &= (\hat{\bm{X}}^{\prime}\bm{X})^{-1} \hat{\bm{X}}^{\prime}\bm{y}
\end{align*}
\end{itemize}
\end{frame}
%---------------------------------------------
\begin{frame}{Instrumental variables: over-identification}
\begin{itemize} 
    \item Projection of columns of $\bm{X}$ in the column space of $\bm{Z}$:
$$
\hat{\bm{X}} = 
\bm{Z}(\bm{Z}^{\prime}\bm{Z})^{-1} \bm{Z}^{\prime}\bm{X},
$$
\item Exogenous columns (regressors) in $\bm{X}$ appear in $\bm{Z}$ as well. Such columns are perfectly replicated in $\hat{\bm{X}}$.
\medskip
\item It may be shown that IVR is equivalent to OLS regression \\$\bm{y} \leftarrow \hat{\bm{X}}$:
\begin{align*}
 \bm{\hat{\beta}}_{\textnormal{IV}} &= 
 (\hat{\bm{X}}^{\prime}\bm{X})^{-1} \hat{\bm{X}}^{\prime}\bm{y} \\
 &= (\bm{X}^{\prime}(\bm{I}-\bm{M}_Z)\bm{X})^{-1} \bm{X}^{\prime}(\bm{I}-\bm{M}_Z)\bm{y} \\
 &= (\hat{\bm{X}}^{\prime}\hat{\bm{X}})^{-1} \hat{\bm{X}}^{\prime}\bm{y}
\end{align*}
\item $\bm{y} \leftarrow \hat{\bm{X}}$ is part of a two-stage LS (2SLS) method, \\(discussed next).
\end{itemize}
\end{frame}
%---------------------------------------------
\begin{frame}{Instrumental variables: identification conditions}
\begin{itemize} 
\item In $\bm{y} = \bm{X\beta}+\bm{u}$, multiple $\bm{x}_j$ regressors may be endogenous.
\medskip
\item Identification (estimability) conditions:
\medskip
\begin{itemize} 
\item \textbf{Order condition:} We need at least as many IVs (excluded exogenous variables) as there are included endogenous regressors in the main (structural) equation.  
\\ \medskip This is a necessary condition for identification. 
\bigskip
\item \textbf{Rank condition:} $\hat{\bm{X}} = 
\bm{Z}(\bm{Z}^{\prime}\bm{Z})^{-1} \bm{Z}^{\prime}\bm{X}$ has full column rank $(k)$ so that $(\hat{\bm{X}}^{\prime}\bm{X})^{-1}$ or $(\hat{\bm{X}}^{\prime}\hat{\bm{X}})^{-1}$ can be calculated in the IV estimator $\bm{\hat{\beta}}_{\textnormal{IV}} =  (\hat{\bm{X}}^{\prime}\bm{X})^{-1} \hat{\bm{X}}^{\prime}\bm{y}$ (will be discussed in detail with respect to 2SLS method and for SEM models).
\\ \medskip This is a necessary and sufficient condition for identification. 
\end{itemize}
\end{itemize}
\end{frame}
%---------------------------------------------
\begin{frame}{2SLS as a special case of IVR}
$$
 \bm{\hat{\beta}}_{\textnormal{IV}} ~=~ 
 (\hat{\bm{X}}^{\prime}\bm{X})^{-1} \hat{\bm{X}}^{\prime}\bm{y} 
 ~=~ (\hat{\bm{X}}^{\prime}\hat{\bm{X}})^{-1} \hat{\bm{X}}^{\prime}\bm{y}
$$
\textbf{2SLS}: 
\begin{itemize}
\item Structural equation\\
\smallskip
$y_1=\beta_0+\beta_1 y_2+\beta_2 x_2+ \dots + \beta_k x_{k} +u \quad | ~z_1 \textnormal{~exists}$\\
\medskip
\item $1^{\textnormal{st}}$ stage of 2SLS: estimate reduced form for $y_2$:\\
\smallskip
$\hat{y}_2=\hat{\pi}_0+\hat{\pi}_1 z_1 + \hat{\pi}_{2} x_{2} + \dots +\hat{\pi}_k x_k$
\medskip
\item $2^{\textnormal{nd}}$ stage of 2SLS: Use $\hat{y}_2$ to estimate structural equation:\\
\smallskip
$y_1=\beta_0+\beta_1 \hat{y}_2 +\beta_2 x_2+ \dots + \beta_k x_{k} +u$
\smallskip
\item Note that RHS in the $2^{\textnormal{nd}}$ stage contains all exogenous regressors repeated from $\bm{X}$, while $\hat{y}_2$ is $y_2$ ``projected'' onto $\bm{Z}$ and thus uncorrelated with $u$.
\item Order condition explained: if $\pi_1=0$, $\hat{y}_2$ is a perfect linear combination of the remaining RHS regressors in $2^{\textnormal{nd}}$ stage.
\end{itemize}
\end{frame}
%---------------------------------------------
\begin{frame}{Two stage least squares}
\textbf{2SLS properties}
\vspace{0.3cm}
\begin{itemize}
\item The standard errors from the OLS second stage regression are biased and inconsistent estimators with respect to the original structural equation (SW handles this problem automatically).
\vspace{0.3cm}
\item If there is one endogenous variable and one instrument then 2SLS = IV
\vspace{0.3cm}
\item With multiple endogenous variables and/or multiple instruments, 2SLS is a special case of IVR.
\vspace{0.3cm}
\end{itemize}
\end{frame}
%---------------------------------------------
\begin{frame}{Two stage least squares}
\textbf{Statistical properties of the 2SLS/IV estimator}
\vspace{0.3cm}
\begin{itemize}
\item Under assumptions completely analogous to OLS, but conditioning on $z_i$ rather than on $x_i$ , 2SLS/IV is consistent and asymptotically normal.
\vspace{0.3cm}
\item 2SLS/IV estimator is typically much less efficient than the OLS estimator because there is more multicollinearity and less explanatory variation in the second stage regression
\vspace{0.3cm}
\item Problem of multicollinearity is much more serious with 2SLS than with OLS
\vspace{0.3cm}
\item Corrections for heteroskedasticity/serial correlation analogous to OLS
\vspace{0.3cm}
\item 2SLS/IV estimamtion easily extends to time series and panel data situations
\end{itemize}
\end{frame}
%---------------------------------------------
\section{IV tests: introduction}
\begin{frame}{IV tests: introduction}
LRM: $y_{i1}=\beta_0+\beta_1 y_{i2}+\beta_2 x_{i1}+u_i$; \quad $\bm{z}$ instruments exist \\
\vspace{0.3cm}
\underline{IV regression advantages} for endogenous $y_2$:
\vspace{-0.2cm}
\begin{align*}
& \rightarrow \hat{\beta}_{1,\textnormal{OLS}} \enspace \parbox[t]{24em}{ is a \textcolor{red}{biased and inconsistent estimator} \\(asymptotic errors)}\\
 & \rightarrow \hat{\beta}_{1, \textnormal{IV}} \enspace \parbox[t]{24em}{ is a \textcolor{blue}{biased and consistent estimator} (increased \\sample size (\textit{n}) lowers estimator bias and s.e.)} 
\end{align*}
\underline{IVR disadvantages (price for the IV regression)}:
\begin{itemize}
\item $\textnormal{s.e.}(\hat{\beta}_{1, \textnormal{IV}}) > \textnormal{s.e.}(\hat{\beta}_{1,\textnormal{OLS}})$
\item $\hat{\beta}_{1, \textnormal{IV}}$ is biased, even if $y_2$ is actually exogenous\\
\smallskip
$\hat{\beta}_{1, \textnormal{OLS}}$ is unbiased for exogenous regressors \\ 
\small (potentially, pending other G-M conditions).
\end{itemize}
\end{frame}
%---------------------------------------------
\begin{frame}{IV tests: introduction}
LRM: $y_{i1}=\beta_0+\beta_1 y_{i2}+\beta_2 x_{i1}+u_i$; \quad $\bm{z}$ instruments exist
\medskip
\begin{itemize}
\item Is the regressor $y_2$ endogenous / $\mathrm{corr}(y_2, u) \neq 0$ / ? \\
Is it meaningful to use IVR (considering IVRs ``price'')?\\
\textbf{Durbin-Wu-Hausman endogeneity test}\\
\item Are the instruments actually helpful\\
(weakly or strongly correlated with endogenous regressors)? \\
\textbf{Weak instruments test}\\
\item Are the instruments really exogenous / $\mathrm{corr}(z_j, u)=0$ / ?\\
\textbf{Sargan test} (only applicable in case of over-identification)\\
\end{itemize}
\medskip
\small Different types \& specifications for IV-tests exist, often focusing on the distribution of the difference between IVR and OLS estimators ($\bm{\hat{\beta}}_{\textnormal{IV}}-\bm{\hat{\beta}}_{\textnormal{OLS}}$) under the corresponding $H_0$.
\end{frame}
%---------------------------------------------
\begin{frame}{Durbin-Wu-Hausman endogeneity test}
$$y_{i1}=\beta_0+\beta_1 y_{i2} + \beta_2 x_{i1} +  u_i ~~~~|~z_{i1}, \qquad \qquad (1)$$
\medskip
\textbf{\underline{DWH test motivation:}} \\If $z_1$ is a proper instrument (uncorrelated with $u$), then $y_2$ is endogenous (correlated with $u$) if and only if $\varepsilon$ (error from reduced form equation) is correlated with $u$. \\
\medskip
\begin{itemize}
\item $y_2$ in \eqref{eq:one} is endogenous ~~$\Leftrightarrow ~~\textnormal{corr}(y_2,u)\neq0$
\item Reduced form: $y_2 = \textit{l.f.}(x_1, z_1) + \varepsilon ~\Rightarrow~ y_2 = \hat{y}_2 + \hat{\varepsilon}$
\item $\textnormal{corr}(y_2,u)\neq0 ~\wedge~\textnormal{corr}(\hat{y}_2,u) = 0 ~\Rightarrow~ \textnormal{corr}(\hat{\varepsilon},u)\neq0 $
\item $y_1$ is always correlated with $u$ in \eqref{eq:one}. 
\item Hence, $\hat{\varepsilon}$ is significant in the regression, if $y_2$ is endogenous.
\item IV/IVs uncorrelated with $u$ is essential for DWH to ``work''.
\end{itemize}
\bigskip
\textbf{\underline{Note:}} other versions of the DWH test exist...
\end{frame}
%---------------------------------------------
\subsection{Durbin-Wu-Hausman (endogeneity in regressors)}
\begin{frame}{Durbin-Wu-Hausman endogeneity test}
Structural equation: 
\begin{align}
\label{eq:one} y_{i1}=\beta_0+\beta_1 y_{i2} + \beta_2 x_{i1} + u_i; \quad \text{IVs: $z_1$ and $z_2$}
\end{align}
Reduced form for $y_2$: 
\begin{align}
\label{eq:two} y_{i2}=\pi_0 + \pi_1 z_{i1} + \pi_2 z_{i2} + \pi_3 x_{i1} + \varepsilon_i
\end{align}
\vspace{-0.4cm}
\begin{itemize}
\item[$H_0$:] $y_2$ is exogenous $\leftrightarrow$ $\hat{\varepsilon}$ is not significant when added to equation \eqref{eq:one}
\item[$H_1$:] $y_2$ is endogenous $\rightarrow$ OLS is not consistent for \eqref{eq:one} estimation, use IVR (2SLS).
\end{itemize}
\textbf{\underline{Testing algorithm:}}
\begin{enumerate}
\item Estimate equation \eqref{eq:two} and save residuals $\hat{\varepsilon}$.
\item Add residuals $\hat{\varepsilon}$ into equation \eqref{eq:one} and estimate using OLS\\ (use HC inference).
\item $H_0$ is rejected if $\hat{\varepsilon}$ in the modified equation \eqref{eq:one} is statistically significant ($t$-test).
\end{enumerate}
\end{frame}
%---------------------------------------------
\subsection{Weak instruments test}
\begin{frame}{Weak instruments}
\textbf{Motivation for Weak instruments and Sargan tests:} \\
\medskip
LRM: \quad $y_{i1}=\beta_0+\beta_1 y_{i2}+\beta_2 x_{i1}+u_i$; \ $z$ instrument exists
\bigskip
\begin{itemize}
\item IVR is consistent if $\mathrm{cov}(z,y_2) \neq 0$ and $\mathrm{cov}(z, u) = 0$
\item If we allow for (weak) correlation between $z$ and $u$, the asymptotic error of IV estimator is:
\small $$\mathrm{plim}(\hat{\beta}_{1, IV}) = \beta_1 + \frac{\mathrm{corr}(z,u)}{\mathrm{corr}(z,y_2)} \cdot \frac{\sigma_u}{\sigma_{y_2}} $$
%BOX
\item If $\mathrm{corr}(z, y_2)$ is too weak (too close to zero in absolute value), OLS may be better than IV. The asymptotic bias for OLS (LRM with endogenous $y_2$):
\small $$\mathrm{plim}(\hat{\beta}_{1,OLS})=\beta_1+\mathrm{corr}(y_2,u) \cdot \frac{\sigma_u}{\sigma_{y_2}}$$
\end{itemize}
Rule of thumb: IF $|\mathrm{corr}(z,y_2)| < |\mathrm{corr}(y_2,u)|$, do not use IVR.
\end{frame}
%---------------------------------------------
\begin{frame}{Weak instruments}
Structural equation: 
\begin{align*}
y_1=\beta_0+\beta_1 y_2 + \beta_2 x_1 + \dots + \beta_{k+1} x_k + u; \quad \text{IVs: $z_1$, $z_2$, \dots , $z_m$}
\end{align*}
The reduced form for $y_2$: 
\begin{align*}
y_2=\pi_0 + \pi_1 x_1 + \pi_2 x_2 + \dots + \pi_k x_k + \theta_1 z_1 + \dots + \theta_m z_m + \varepsilon
\end{align*}
\vspace{-0.3cm}
\begin{itemize}
\item[$H_0$:] $\theta_1=\theta_2=\dots=\theta_m=0$ \\interpretation: ``instruments are weak''.
\item[$H_1$:] $\neg$ $H_0$
\end{itemize}
\bigskip
\textbf{\underline{Testing for weak instruments:}} \\
\medskip
Use $F$-test (heteroskedasticity-robust) or the $LM$ test ($\chi^2$) to test for the joint null hypothesis.
\end{frame}
%---------------------------------------------
\subsection{Sargan (exogeneity in IVs, over-identification only)}
\begin{frame}{Sargan test (over-identification only)}
Structural equation: 
\begin{align}
\label{eq:three} y_{i1}=\beta_0+\beta_1 y_{i2} + \beta_2 x_{i1} + u_i; \quad \text{IVs: $z_1$, $z_2$, \dots}
\end{align}
\vspace{-0.2cm}
\begin{itemize}
\item[$H_0$:] all IVs are uncorrelated with $u$
\item[$H_1$:] at least one instrument is endogenous
\end{itemize}
\bigskip
\textbf{\underline{Testing algorithm:}}
\begin{enumerate}
\item Estimate equation \eqref{eq:three} using IVR and save the $\hat{u}$ residuals.
\item Use OLS to estimate auxiliary regression: \ $\hat{u} \leftarrow f(\bm{x, z})$ and save the $R_a^2$
\item Under $H_0$: $nR_a^2 \sim \chi_q^2$ where \\$q =$ (number of IVs) - (number of endogenous regressors) 
 \\i.e. $q$ is the number of over-identifying variables.
\item If the observed test statistic exceeds its critical value \\(at a given significance level), we reject $H_0$.
\end{enumerate}
\end{frame}
%---------------------------------------------
\subsection{IV tests: example}
\begin{frame}[fragile]{IV tests: example}
%\begin{minipage}[t]{.3\textwidth}

\tiny
\begin{lstlisting}[escapechar=\%]
Call:
ivreg(formula = lbwght ~  %\mytikzmark{REG}{packs + male}% |  %\mytikzmark{IV}{faminc + motheduc + male}%, 
    data = bwght)

Residuals:
     Min       1Q   Median       3Q      Max 
-1.66291 -0.09793  0.01717  0.11616  0.82793 

Coefficients:
            Estimate Std. Error t value Pr(>|t|)    
(Intercept)  4.77419    0.01099 434.478  < 2e-16 ***
packs       -0.25584    0.07613  -3.361 0.000798 ***
male         0.02422    0.01048   2.311 0.021003 *  

Diagnostic tests:
                  df1  df2 statistic p-value    
Weak instruments    2 1383    38.732  <2e-16 %\mytikzmark{1}{***}%
Wu-Hausman          1 1383     5.385  0.0205 %\mytikzmark{2}{* }% 
Sargan              1   NA     4.476  0.0344 %\mytikzmark{3}{*  }%
---
Signif. codes:  0 *** 0.001 ** 0.01 * 0.05 . 0.1  

Residual std. error: 0.195 on 1384 d.f.
Multiple R-Squared: -0.04371,	Adj R-sqr: -0.04522 
Wald test: 8.342 on 2 and 1384 DF,  p-value: 0.0002504 
\end{lstlisting}
%\end{minipage}

\begin{tikzpicture}[<-,overlay,remember picture,inner sep=0.6pt,shorten <=0.2em,font=\tiny]
\tikzset{
    mynode/.style={rectangle, draw=Red,fill=white, minimum size=1cm, text width=4.2cm, font=\scriptsize},
    mynode2/.style={rectangle, minimum size=.5cm, text width=3cm},
    mynode3/.style={rectangle, minimum size=.5cm, text width=.5cm},
    myarrow/.style={->, >=stealth, semithick, Red},
    myarrow2/.style={->, >=stealth, semithick, Blue}
}
\node[mynode] at (9,7)(box){Wooldridge, bwght dataset\\ R code, \{AER\} package};
\node[mynode3] at (11,5.5)(box4){IVs};
\node[mynode2] at (10.2,5)(box5){Regressors \\explicitly included \\in equation};
\node[mynode2] at (10.2,3)(box1){\checkmark Reject $H_0$: \\IVs are weak};
\node[mynode2] at (10.2,2)(box2){\checkmark Reject $H_0$: \\pack are exogenous};
\node[mynode2] at (10.2,1)(box3){!! Reject $H_0$: all IVs \\ are uncorrelated with $u$ \\ (!DWH assumptions!)};
\draw[myarrow] (box1) -- ++ (1);
\draw[myarrow] (box2) -- ++ (2);
\draw[myarrow] (box3) -- ++ (3);
\draw[myarrow2] (box4) -- ++ (IV);
\draw[myarrow2] (box5) -- ++ (REG);
\end{tikzpicture}

\end{frame}


\end{document}