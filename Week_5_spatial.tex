%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Beamer Presentation
% LaTeX Template
% Version 1.0 (10/11/12)
%
% This template has been downloaded from:
% http://www.LaTeXTemplates.com
%
% License:
% CC BY-NC-SA 3.0 (http://creativecommons.org/licenses/by-nc-sa/3.0/)
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\documentclass{beamer}

\mode<presentation> {
\usetheme{Madrid}
\usefonttheme{serif} 
\setbeamertemplate{navigation symbols}{} 
}
\usepackage{graphicx} % Allows including images
\usepackage{booktabs} % Allows the use of \toprule, \midrule and \bottomrule in tables
\usepackage[T1]{fontenc}
\usepackage[utf8]{inputenc}
\usepackage{amsmath}
\usepackage{color}
%\usepackage[czech]{babel}
\usepackage{lmodern}  
\usepackage{rotating}
\usepackage{scrextend}
\usepackage{pifont}
\usepackage{hyperref}
\usepackage{bm}

%----------------------------------------------------------------------------------------
%	TITLE PAGE
%----------------------------------------------------------------------------------------

\title[Týden 5]{Praktikum z ekonometrie} % The short title appears at the bottom of every slide, the full title is only on the title page

\author{VŠE Praha} % Your name
\institute[4EK417] % Your institution as it will appear on the bottom of every slide, may be shorthand to save space
{
% Your institution for the title page
\medskip
\textit{Tomáš Formánek} % Your email address
}
\date{} % Date, can be changed to a custom date

\begin{document}

\begin{frame}
\titlepage % Print the title page as the first slide
\end{frame}

\begin{frame}
\frametitle{Outline} % Table of contents slide, comment this block out to remove it
\tableofcontents % Throughout your presentation, if you choose to use \section{} and \subsection{} commands, these will automatically be printed on this slide as an overview of your presentation
\end{frame}
%	PRESENTATION SLIDES
%---------------------------------------------------------------------
\section{Introduction}
\begin{frame}{Introduction}
\end{frame}
%---------------------------------------------------------------------
\begin{frame}{Introduction - spatial analysis}
\begin{figure}
	\includegraphics[width=.9\textwidth]{IMG/sp_auto2.PNG}
\end{figure}
\end{frame}
%---------------------------------------------------------------------
\begin{frame}{Introduction - spatial analysis}
Methods of quantitative spatial analysis:\\
\bigskip
\begin{itemize}
    \item Visualization \\Maps, graphical display
    \bigskip
    \item Data exploration \& descriptive methods \\Tools to broadly look at spatial patterns
    \bigskip
    \item Econometric modeling \\Fitting models, testing hypotheses, formalizing spatial dependence, discerning spatial effects from other factor (e.g. macroeconomic)
\end{itemize}
\end{frame}
%---------------------------------------------------------------------
\begin{frame}{Introduction - spatial analysis}
 What are spatial data?\\
\bigskip
\begin{itemize}
    \item Data that are location speciffic and that vary in space.
    \medskip
    \item Referenced by a spatial location $\bm{s}$ (usually 2D),  \\$\bm{s} = (x; y)$; $x$ is longitude (easting) and $y$ is latitude (northing). \\ \medskip
    May also be referenced by a zip code, county or state ID.
    \medskip
    \item Data that are close together in space (time) are often more alike than those that are far apart.
    \medskip
    \item Tobler's first law of geography: \\ ``Everything is related to everything else, but near things are more related than distant things.''
\end{itemize}
\end{frame}
%---------------------------------------------------------------------
\begin{frame}{History of spatial analysis: 1854 -- London -- cholera}
\vspace{-0.5cm}
\begin{figure}
	\includegraphics[width=.65\textwidth]{IMG/sp_cholera.eps}
\end{figure}
\end{frame}
%---------------------------------------------------------------------
\begin{frame}{History of spatial analysis: 1854 -- London -- cholera}
 Dr. John Snow: Early spatial analysis\\
\medskip
\begin{itemize}
    \item In August 1854, there was a major Cholera outbreak in the Soho neighbourhood of London, UK. There were 127 cholera related deaths around the area.
    \smallskip
    \item At the time, germ theory (microorganisms causing disease) was not generally accepted. Dr. J. Snow was a MD, pioneer of germ theory and a statistician.
    \smallskip
    \item Dr. John Snow spoke to local residents and mapped where cholera cases occurred. As a result of his map, he was able to pinpoint the public water pump on Broad Street as the source of contaminated water causing the cholera outbreak.
    \smallskip
    \item Dr. Snow used statistics to find a relationship between water sources and cholera cases and subsequently found out that the waterworks company supplying water to Broad Street pump was taking water from a sewage polluted area of the Thames river.
\end{itemize}
\end{frame}
%---------------------------------------------------------------------
\begin{frame}{History of spatial analysis: 1935 -- field experiments}
\vspace{-0.5cm}
\begin{figure}
	\includegraphics[width=.8\textwidth]{IMG/sp_Fisher.jpg}
\end{figure}
\end{frame}
%---------------------------------------------------------------------
\begin{frame}{History of spatial analysis: 1935 -- field experiments}
 R.A. Fisher: Early spatial analysis\\
\medskip
\begin{itemize}
    \item R.A. Fisher was probably the first to recognize implications of spatial dependency for statistical analysis.
    \smallskip
    \item In his work on design of experiments in agricultural science, he wrote (Fisher, 1935, p. 66):\\
    ``After choosing the area we usually have no guidance beyond the widely verified fact that patches in close proximity are commonly more alike, as judged by the yield of crops, than those which are further apart.''
    \smallskip
    \item Observed spatial variability, i.e. field-to-field variability, was largely due to physical properties of the soil and environmental properties of the field. He avoided the confounding of treatment effects with plot effect with the introduction of randomization.
    \smallskip
    \item Fisher's solution was to eliminate spatial dependency bias by localizing the crops under scrutiny into randomly assigned blocks.
\end{itemize}
\end{frame}
%---------------------------------------------------------------------
\section{Spatial stochastic processes}
\begin{frame}{Spatial stochastic processes}
\end{frame}
%---------------------------------------------------------------------
\begin{frame}{Measuring spatial variables}
Spatial data: measurements and measurement scales
\begin{itemize}
    \item \textbf{Generally, data vary continuously over space, but are measured only at discrete locations.} 
    \begin{itemize}
        \item To characterize spatial variables, spatial aggregation is necessary.
        \item Aggregation of spatial variables may be just another source of bias and potential data mis-manipulation\\ 
        Summary values are influenced by shape and scale of spatial units.\\
        Shape (administrative boundaries) may change over time.
    \end{itemize}
    \item Scale, consistency and relevance should be carefully considered
when collecting and analyzing spatial data
\end{itemize}
\medskip
Spatio-temporal data:
\begin{itemize}
    \item Data that are location specific and repeated in time.
    \item Each variable-observation has a location, time and value.
    \item Similar methods for analysis, with an added time dimension \\(choice of sampling frequency, spatial panel data analysis)
\end{itemize}
\end{frame}
%---------------------------------------------------------------------
\begin{frame}{Measuring spatial distances}
Distances ($d$) can be defined in a variety of ways, yet the following technical conditions should always apply (invariant to spatial translation, i.e. ``shift''):\\
\medskip
\begin{enumerate} 
\item[1] $d(\bm{s}_i, \bm{s}_j) = d(\bm{s}_j, \bm{s}_i)$ \\ \smallskip (symmetry) 
\medskip
\item[2] $d(\bm{s}_i, \bm{s}_i) = 0$  \\ \smallskip (dist. between a point and itself is zero) 
\medskip
\item[3] $d(\bm{s}_i, \bm{s}_j) \leq d(\bm{s}_i) + d(\bm{s}_j)$  \\ \smallskip (triangle inequality; 
$d(\bm{s}_i)$ is the distance from origin) 
\end{enumerate}
\end{frame}
%---------------------------------------------------------------------
\begin{frame}{Measuring spatial distances}
Distances ($d$) can be defined in a variety of ways, yet the following technical conditions should always apply (invariant to spatial translation, i.e. ``shift''):\\
\medskip
\begin{enumerate} 
\item[1] $d(\bm{s}_i, \bm{s}_j) = d(\bm{s}_j, \bm{s}_i)$ \\ \smallskip (symmetry) 
\medskip
\item[2] $d(\bm{s}_i, \bm{s}_i) = 0$  \\ \smallskip (dist. between a point and itself is zero) 
\medskip
\item[3] $d(\bm{s}_i, \bm{s}_j) \leq d(\bm{s}_i) + d(\bm{s}_j)$  \\ \smallskip (triangle inequality; 
$d(\bm{s}_i)$ is the distance from origin) 
\end{enumerate}
\end{frame}
%---------------------------------------------------------------------
\begin{frame}{Measuring spatial distances}
\small{\textbf{Euclidean distances}, measured between two point in the ``ordinary'' Euclidean space. In 2D, the Euclidean distance ($L_2$ norm) is defined as 
$$
d(\bm{s}_i, \bm{s}_j) = \sqrt[]{(s_{ix}-s_{jx})^2+(s_{iy}-s_{jy})^2}\,,
$$
where the $x$ and $y$ subscripts handle planar coordinates. For smaller distances, the computational simplicity is attractive.\\
\medskip
\textbf{Great circle distances} - for larger distances, planar projection accumulates non-negligible errors. The shortest path between two points on a sphere (given their longitudes and latitudes): 
$$
d(\bm{s}_i, \bm{s}_j) = 2r \, \arcsin \,
 \sqrt{ \sin^2 \left( \frac{\phi_j-\phi_i}{2} \right)
       + \cos(\phi_i) \cos(\phi_j)
       \sin^2 \left( \frac{l_j-l_i}{2} \right) }\,,
$$
where $r$ is the radius of the sphere, $\phi_1$ and $\phi_2$ are the latitudes of $\bm{s}_i$ and $\bm{s}_j$ in radians, $l_i$ and $l_j$ are the longitudes (in radians). Its only an approximation when applied to the Earth, which is not a perfect sphere (correct within a 0.5\%; alternative: Vicenty's formulae).\\
\medskip
\textbf{Manhatan distances} - $L_1$ norm, a function on a fixed grid.
}
\end{frame}
%---------------------------------------------------------------------
\begin{frame}{Spatial stochastic processes}
For a generic location $\bm{s}$ given by a vector of $d$ coordinates in a $d$-dimensional Euclidean space, spatial stochastic process \\(``random field'') is often denoted as
$$ Z(\bm{s}): \bm{s} \in D \subseteq \mathbb{R}^d \, .$$
\vspace{-0.5cm}
\begin{itemize}
    \item Typically, $d=2$ for most economic and econometric applications, $d=3$ is often used in fields such as geology or astronomy. 
    \smallskip
    \item $D$ is a fixed finite set of $N$ spatial locations $\bm{s}_1, \bm{s}_2, \dots, \bm{s}_N$. 
    \smallskip
    \item Individual $\bm{s}_i$ units are points in space (say, with GPS-based latitude and longitude coordinates). Sometimes, such points can be associated with non-zero surface area elements.
    \item Much like in time-series analysis, the individual realizations of a spatial stochastic process -- random field -- are often denoted \\$z(\bm{s}_i)$ or, simply, $z_i$.
\end{itemize}
\end{frame}
%---------------------------------------------------------------------
\begin{frame}{Spatial stochastic processes}
Stationarity is a common assumption: a spatial process under scrutiny repeats itself over the domain $D$. If we translate the entire set of coordinates by $\bm{h}$ -- a specific distance in a specified direction, the stochastic process and its features remain unchanged.\\
\medskip
\textbf{Strong stationarity} of a random filed: We start with a finite-dimensional distribution: 
$$F_{\bm{s}_1, \dots,\bm{s}_m }(z_1,\dots, z_m) = P[Z(\bm{s}_1) \leq z_1, Z(\bm{s}_2) \leq z_2, \dots, Z(\bm{s}_m) \leq z_m] \, .$$  
Strong stationarity $\leftrightarrow$~$F$ is invariant under spatial translation $\bm{h}$. Unlike $d_{ij}$ (Euclidean distance between two spatial units $\bm{s}_i$ and $\bm{s}_j$), \\$\bm{h}$ is an orientated distance ``shift'' (spatial translation) vector. \\For strong stationarity:
\begin{equation*}
\begin{aligned}
& P \left[Z(\bm{s}_1) \leq z_1, Z(\bm{s}_2) \leq z_2, \dots, Z(\bm{s}_m) \leq z_m \right] \\
& = P \left[Z(\bm{s}_1+\bm{h}) \leq z_1, Z(\bm{s}_2+\bm{h}) \leq z_2, \dots, Z(\bm{s}_m+\bm{h}) \leq z_m \right] \, .
\end{aligned} 
\end{equation*}
\end{frame}
%---------------------------------------------------------------------
\begin{frame}{Spatial stochastic processes}
\vspace{-0.2cm}
\textbf{Weak  stationarity} (also called second order stationarity) assumes that the first two moments exist, are invariant (and finite) and covariance only depends on spatial translation (orientated distance) $\bm{h}$:
\begin{equation*}
\begin{aligned}  
E[Z(\bm{s})] &= \mu \,, \\
\textnormal{var}[Z(\bm{s})] & = \sigma^2 \,, \\
\textnormal{cov} [Z(\bm{s}+\bm{h}),Z(\bm{s})] = C(\bm{s}+\bm{h}, \bm{s}) & = C(\bm{h}) \,.
\end{aligned} 
\end{equation*}
As autocovariance is a function of $\bm{h}$ only (under weak st.), \\for any spatial points $\bm{s}_i$ and $\bm{s}_j$ such that $\bm{s}_i -\bm{s}_j = \bm{h}$, we can write: 
\begin{equation*}
\textnormal{cov} \left[ Z(\bm{s}_i), Z(\bm{s}_j) \right] = C(\bm{s}_i - \bm{s}_j) = C(\bm{h}) \,.  
\end{equation*}
Covariogram $C(\bm{h})$ is the covariance between two spatial units, separated by $\bm{h}$. For $\bm{h} = \bm{0}$, it simply describes variance: 
$$\textnormal{cov}\,[Z(\bm{s}+\bm{0}),Z(\bm{s})] =  C(\bm{0}) = \textnormal{var}\,[Z(\bm{s})]\,.$$
Under weak dependency, covariance disappears with growing distance: 
$$C(\bm{h}) \rightarrow 0~\textnormal{as}~||\bm{h}|| \rightarrow \bm{\infty}$$.
\end{frame}
%---------------------------------------------------------------------
\begin{frame}{Spatial stochastic processes}
\textbf{Intrinsic stationarity} is less restrictive than weak (second order) stationarity and it is defined in terms of first differences. \\
\smallskip
A spatial process is intrinsically stationary if the difference between two observed spatial points is weakly stationary: 
\begin{equation*} 
\begin{aligned}  
E[Z(\bm{s}+\bm{h})-Z(\bm{s})] &= 0 \,, \\
\textnormal{var}[Z(\bm{s}+\bm{h})-Z(\bm{s})] & = 2\gamma(\bm{h}) \,,
\end{aligned} 
\end{equation*}
where $2\gamma(\bm{h}) \geq 0$ is the variogram. Generally, $2\gamma(\bm{h})$ increases with growing oriented distance $\bm{h}$. \\
\smallskip
The two types of relaxed stationarity are related: weak stationarity implies intrinsic stationarity but not vice versa. For weakly stationary spatial processes (where $E(Z(\bm{s}+\bm{h}))=E(Z(\bm{s}))=\mu$) the variogram simplifies to: 
\begin{equation*}
2\gamma(\bm{h}) = E \left[ \left( Z(\bm{s}+\bm{h}) - Z(\bm{s})  \right)^2 \right],
\end{equation*}
i.e. to the expected squared difference between two observed realizations of a spatial stochastic process. 
\end{frame}
%---------------------------------------------------------------------
\begin{frame}{Spatial stochastic processes}
\textbf{Semivariogram} is denoted as $\gamma(\bm{h})$ and it equals to half the variogram. \\ \smallskip Since  $2\gamma(\bm{h})$ is calculated as expectation of a square, $\gamma(\bm{h}) \geq 0$ for both weakly and intrinsically stationary random fields. \\ \smallskip Also, at $\bm{h}=\bm{0}$, $\gamma(\bm{0}) = 0$ because 
$$ E \left[ \left( Z(\bm{s}_i) - Z(\bm{s}_i)  \right)^2 \right]=0 \textnormal{~for~} \forall \, i\,.$$
Variogram (semivariogram) is a generalization of the covariogram $C(\bm{h})$ and under weak stationarity, the two functions are related by: 
\begin{equation*}
    \gamma(\bm{h}) = C(\bm{0}) - C(\bm{h})\,.
\end{equation*}
If a stationary stochastic process has no spatial dependency at all \\(i.e. $C(\bm{h})=0$ for $\bm{h}\not =\bm{0}$), the semivariogram is constant: $\gamma(\bm{h}) = \textnormal{var}[Z(\bm{s})]$ everywhere, except for $\bm{h}=\bm{0}$, where $\gamma(\bm{0})=0$. 
\end{frame}
%---------------------------------------------------------------------
\begin{frame}{Spatial stochastic processes}
\textbf{Isotropic spatial process} may be defined through a semivariogram:\\  $$\gamma(\bm{h})=\gamma(||\bm{h}||)=\gamma(d).$$ 
Isotropy means that the semivariogram depends only on the distance $d$ between two points and not on direction.\\ \medskip The lack of isotropy -- anisotropy -- means the semivariogram depends on direction as well as distance. \\ \medskip To assess and test anisotropy, we can estimate and plot directional semivariograms (shown next).
\end{frame}
%---------------------------------------------------------------------
\begin{frame}{Spatial stochastic processes}
\textbf{Empirical semivariogram} \\
To perform empirical analysis of distance-based data correlations, we construct the so called empirical semivariogram. First, we divide the distances observed over the domain $D$ into $K$ conveniently chosen intervals: 
$$I_1 = (0, d_1 ], \, I_2 = (d_1, d_2 ], \, \dots \, , \, I_K = (d_{K-1}, d_K ] \, .$$
Here, $d_1$ is the maximum distance within the $I_1$ interval and $d_K$ is the maximum distance observed over the field of data. \\ \smallskip The intervals can be proportional in terms of distance or in terms of sets of observation pairs allocated to each interval (to adjust for unevenly spaced observations). \\ \smallskip Note that distances are determined by $d$ (distance magnitudes) only -- here, we do not use the orientated distances $\bm{h}$.
\end{frame}
%---------------------------------------------------------------------
\begin{frame}{Spatial stochastic processes}
\textbf{Empirical semivariogram} is calculated using the following formula:
\begin{equation*}
\hat{\gamma}(d_k) = 
\frac{1}{2N(d_k)} \sum_{N(d_k)}[Z(\bm{s}_i)-Z(\bm{s}_j)]^2 \,,
\end{equation*}
where $N(d_k)$ is the number of distinct observation pairs in the interval $I_k$ and $\hat{\gamma}(d_k)$ is the semivariogram estimate for its corresponding group (interval) of distances. \\
\bigskip
Usually, we fit a convenient parametric function (exponential, spherical, Gaussian, etc.) to the estimated $\hat{\gamma}(d_k)$ values (shown next). \\ 
\bigskip
The main goal of empirical semivariogram construction is to estimate and visualize the spatial autocorrelation structure of the observed stochastic process.
\end{frame}
%---------------------------------------------------------------------
\begin{frame}{Empirical semivariogram}
\vspace{-0.5cm}
\begin{figure}
	\includegraphics[width=.8\textwidth]{IMG/sp_svgm.jpg}
\end{figure}
\end{frame}
%---------------------------------------------------------------------
\begin{frame}{Empirical semivariogram}
Three main features of an estimated empirical semivariogram: 
\begin{itemize}
\item \textit{Nugget} (nugget effect) describes the micro-scale variations or measurement errors in data. Theoretically, at zero distance, $\gamma(0)=0$. However, two factors play a role here: First, $\gamma(d_1)$ is estimated over the $N(d_1)$ set of pairs, i.e. for the first interval where $ d_{ij} \in (0, d_1 ]$. Second, fitting the empirical semivariogram curve to observed values often causes the non-zero nugget.
\smallskip
\item \textit{Sill} amounts to $\lim_{d \to \infty} \gamma(d)$. The sill corresponds to variance of the stochastic field at distances where spatial dependency (which reduces $\gamma(d)$) no longer applies. $\lim_{d \to \infty} \gamma(d) = C(\bm{0}) = \textit{var}[Z(\bm{s})]$.
\smallskip
\item \textit{Range} is the spatial distance (if any) beyond which the data are not autocorrelated. In a way, range describes the strength of spatial structure -- based on where the semivariogram ``reaches'' its asymptote (sill).
\end{itemize}
\end{frame}
%---------------------------------------------------------------------
\begin{frame}{Empirical semivariogram (fitting)}
\vspace{-0.25cm}
\begin{figure}
	\includegraphics[width=.4\textwidth]{IMG/sp_svgm2.jpg}
\end{figure}
\end{frame}
%---------------------------------------------------------------------
\begin{frame}{Empirical semivariogram (directional semivariogram)}
\begin{figure}
	\includegraphics[width=.8\textwidth]{IMG/sp_svgm3.jpg}
\end{figure}
\end{frame}
%---------------------------------------------------------------------
\begin{frame}{Spatio-temporal stochastic processes}
The above discussion can be generalized to accommodate processes that are observed repeatedly over time. \\ \medskip 
Such observations usually exhibit both spatial and temporal dependency and variability.\\ \medskip  
Given the frequency and density limitations of empirical measurements in continuous space and time, we model our observations as realizations of a spatio-temporal random function (random field)
\begin{equation*} 
Z(\bm{s},t), \hspace{1cm} \textnormal{where~} (\bm{s},t) \in \, \mathbb{R}^d \! \times \mathbb{R} \,,
\end{equation*}
the spatio-temporal domain is indexed in space by $\bm{s} \in \mathbb{R}^d$ and in time by $t \in \mathbb{R}$. \\ \medskip 
The separation between spatial and time dimensions is substantial, which is reflected in the notation.
\end{frame}
%---------------------------------------------------------------------
\begin{frame}{Spatio-temporal stochastic processes}
Weak and intrinsic stationarity concepts can be easily expanded from spatial to spatio-temporal data. \\ \medskip
For an intrinsically stationary process $Z(\bm{s},t)$, spatio-temporal semivariograms (STSV) is:
\begin{equation*} 
\gamma(\bm{h};t) = \frac{1}{2} \textnormal{var} \left[ Z(\bm{s}_0+\bm{h} \, ; \> t_0 + t) - Z(\bm{s}_0;t_0) \right],
\hspace{1cm} (\bm{h},t) \in \, \mathbb{R}^d \! \times \mathbb{R}.
\end{equation*}
STSV does not depend on the selection of origin $(\bm{s}_0, t_0) \in \mathbb{R}^d \! \times \mathbb{R}$ (under intrinsic stationarity). \\ \medskip Also, for intrinsically stationary random fields $Z(\bm{s},t)$, the STSV $\gamma(\bm{h};t)$ is non-negative and $\gamma(\bm{0};0) = 0$.
\end{frame}
%---------------------------------------------------------------------
\begin{frame}{Empirical STSV (EU's Unemp., NUTS0, 2002---2016)}
\begin{figure}
	\includegraphics[width=.7\textwidth]{IMG/sp_STV.eps}
\end{figure}
\end{frame}
%---------------------------------------------------------------------
\section{Data exploration \& descriptive methods}
\begin{frame}{Data exploration \& descriptive methods}
\end{frame}
%---------------------------------------------------------------------
\subsection{Interpolation and Krigging}
\begin{frame}{Interpolation}
\textbf{Inverse distance weighting (IDW):} a deterministic method of interpolation with a known scattered set of points. \\ \smallskip Assigned values to unknown points are calculated with a weighted average of the values available at the known points.
\\ \smallskip 
To find an interpolated value $x_0$ at a given point $\bm{s}_0$ based on samples $x_i=x(\bm{s}_i)$ with $i=1,2,\dots,N$ we use a IDW interpolating function:
$$
x(\bm{s}_0) = 
\begin{cases}
    \frac{\sum_{i=1}^N w_i(\bm{s_0})x_i}{\sum_{i=1}^N w_i(\bm{s_0})}, & 
    \textnormal{if~} d(\bm{s}_0,\bm{s}_i) \neq 0 \textnormal{~for all~}i\\
    x_i & \textnormal{if~} d(\bm{s}_0,\bm{s}_i)=0 \textnormal{~for some~}i \, 
\end{cases} 
$$
where $w_i(\bm{s}_0)=\frac{1}{d(\bm{s}_0,\bm{s}_i)^p}\,$, $\bm{s}_0$ is an interpolating (known) point, $d$ is a distance, and $p$ is a positive \textit{real} number (power parameter). \\ \smallskip
Weight decreases as distance increases from the interpolated points. High $p$ assigns greater influence to closest values -- result may turn into a mosaic of tiles (a Voronoi diagram). 
\end{frame}
%---------------------------------------------------------------------
\begin{frame}{IDW interpolation -- illustration}
\begin{figure}
	\includegraphics[width=.7\textwidth]{IMG/sp_idw.png}
\end{figure}
\end{frame}
%---------------------------------------------------------------------
\begin{frame}{Krigging}
\begin{itemize}
    \item developed by Daniel G. Krige (1919-2013) \\(originally called ``weighted moving averages'' method) 
    \smallskip
    \item Also known as BLUP (best linear unbiased prediction) and closely related to OLS estimation.
    \smallskip
    \item Returns observed values at sampling locations.
    \smallskip
    \item Interpolates values using the intensity and shape of the empirical (semi)variogram. Results depend on the choice of fitting model \\(Gaussian, spherical, exponential, etc.).
    \smallskip
    \item Uses neighborhood and/or distance search radius.
    \smallskip
    \item Provides standard errors of interpolated values.
    \smallskip
    \item Multiple approaches and generalizations exist (Block Krigging). \\
    \url{http://desktop.arcgis.com/en/arcmap/10.3/tools/3d-analyst-toolbox/how-kriging-works.htm}
\end{itemize}
\end{frame}
%---------------------------------------------------------------------
\begin{frame}{Krigging}
\textbf{Kriging}\\ \medskip
\begin{itemize}
    \item $x(\bm{s}_0)= \sum_{i=1}^N \lambda_i \, x(\bm{s}_i)$ ~~s.t.~~ $\sum_{i=1}^N \lambda_i = 1$. 
    \bigskip
    \item In IDW, $\lambda_i$ depend only on distance to the prediction location.\\
    \bigskip
    \item  With krigging, $\lambda_i$ depend on a fitted model to the measured points, the distance to the prediction location, and the spatial relationships among the measured values around the prediction location.
\end{itemize}
\end{frame}
%---------------------------------------------------------------------
\begin{frame}{Krigging}
\textbf{Ordinary kriging} assumes constant unknown mean only over the search neighborhood of $\bm{s}_{0}$. \\ \medskip

\begin{itemize}
    \item Matrix form: $x(\bm{s}_0)= \bm{\lambda}^{\prime} \, \bm{x}$ ~~s.t.~~ $\bm{\lambda}^{\prime} \bm{1} = 1$. \\
    where $\bm{\lambda}^{\prime} = (\lambda_1,\dots,\lambda_N)$ and $\bm{1}$ is a vector of ones.
    \medskip
    \item Krigging -- solve:~~~~ $\min E(x(\bm{s}_0 - \bm{\lambda}^{\prime} \, \bm{x})^2$ s.t. $\bm{\lambda}^{\prime} \bm{1} = 1$.\\
    \medskip
    \item  Minimum variance of $x(\bm{s}_0)$: $~\sigma^2= \sum_{i=1}^N \lambda_i \gamma (\bm{s}_i,\,\bm{s}_0)~+~m \,$
    \medskip
    \item is obtained when $\sum_{i=1}^N \lambda_i \gamma (\bm{s}_i,\,\bm{s}_j)~+~m = \gamma (\bm{s}_j,\,\bm{s}_0)$ for all $j$. \\
    \medskip
    Here, $\gamma$ is the semivarogram, $m$ is an additional LM parameter (mean estimate) that ensures unbiasedness of the estimate. \\For additional discussion and alternative estimation methods \\(simple, universal krigging, etc.), see\\
    \url{https://en.wikipedia.org/wiki/Kriging}
\end{itemize}
\end{frame}
%---------------------------------------------------------------------
\begin{frame}{Krigging -- illustration}
\begin{figure}
	\includegraphics[width=.8\textwidth]{IMG/sp_Krigg.pdf}
\end{figure}
\end{frame}
%---------------------------------------------------------------------
\subsection{Definition of neighbors}
\begin{frame}{Definition of neighbors}
\begin{itemize}
    \item Fotheringham et al (2002): ``Spatial dependency is the extent to which the value of an attribute in one location depends on the values of the attribute in nearby locations.''
    \medskip
    \item Different definitions of spatial dependency are possible.
    \medskip
    \item To discuss spatial dependency, spatial autocorrelation, corresponding tests and spatial econometric models, we need to formalize the concept of \textbf{nearby locations -- neighbors}
\end{itemize}
\end{frame}
%---------------------------------------------------------------------
\begin{frame}{Definition of neighbors}
\begin{itemize}
	\item \textbf{Distance-based approach} defines two units as neighbors if their distance does not exceed some ad-hoc predefined threshold: $\tau$. \\
	\medskip
	\begin{itemize}
		\item Can generate ``islands'' (units with zero neighbors), if $\tau$ is low compared to minimum distances among unit pairs.
		\smallskip
		\item Less suited for analysis of areas with uneven geographic density (of measurements). 
	\end{itemize}
	\medskip
	\item \textbf{Centroids} are used for measuring distances between units with non-zero areas (e.g. regions)\\
	\medskip
	\begin{itemize}
		\item Centroids can be purely geographical, ``main'' city location, population-weighted, transportation-weighted (highway/railway), etc.
	\end{itemize}
\end{itemize}
\end{frame}
%---------------------------------------------------------------------
\begin{frame}{Definition of neighbors}
\begin{figure}
	\includegraphics[width=.7\textwidth]{IMG/sp_neigb.PNG}
	\caption[]{Plot for distance-based neighbors (NUTS2), maximum neighbur distance threshold at 250 km}
\end{figure}
\end{frame}
%---------------------------------------------------------------------
\begin{frame}{Definition of neighbors}
\begin{itemize}
\item \textbf{Contiguity-based approach} spatial units (regions) are neighbors if they share a common border (at least one point).
\bigskip 
\item \textbf{Generalized contiguity approach} is based on the premise that a ``second order'' neighbor is the neighbor of a first order neighbor (the actual contiguous neighbor). \\ \smallskip With this type of approach, we can define a maximum neighborhood lag (order) to control for the highest accepted number
of neighbors traversed (not permitting cycles).	
\end{itemize}
\end{frame}
%---------------------------------------------------------------------
\begin{frame}{Definition of neighbors}
\textbf{$\bm{k}$ Nearest neighbors ($\bm{k}$NN)} \\
\medskip
For each spatial unit, we search for a preset number of $k$ nearest units that we define as its neighbors. 
\medskip
\begin{itemize}
	\item Solves for differences in areal densities ($k$ neighbors are ensured for each unit).
	\smallskip
	\item Usually leads to asymmetric spatial connectivity matrices with potentially flawed neighborhood interpretation. 
	\smallskip
	\item Illustration for $k = 3$   (neighbors only shown for 2 units):
	\begin{figure}
		\includegraphics[width=.2\textwidth]{IMG/sp_neigb2.PNG}
	\end{figure}
\end{itemize}
\end{frame}
%---------------------------------------------------------------------
\subsection{Spatial connectivity and weights matrices}
\begin{frame}{Spatial connectivity matrix ($\bm{C}$)}
$$
~~\bm{C} = \begin{bmatrix}
0 & 1 & 1 & 1 \\
1 & 0 & 1 & 0 \\
1 & 1 & 0 & 1 \\
1 & 0 & 1 & 0
\end{bmatrix}\quad \text{~~a 4-unit example}$$
$$c_{ij}=
	\begin{cases}
	1 & \text{if $i$ and $j$ are neighbors,}\\
	0 & \text{if $i$ and $j$ are not neighbors.}
	\end{cases}$$
\begin{itemize}
	\item Zeros on the diagonal -- units are not neighbors to themselves.
	\smallskip
	\item Spatial connectivity matrix interpretation: 
	\smallskip
	\begin{itemize}
		\item row/column 1: unit 1 is neigbor to units 2,3,4
		\item row/column 2: unit 2 is neigbor to units 1,3 (not 4)
	\end{itemize}
	\smallskip
    \item Matrix $\bm{C}$ is symmetric (for $k$NN, transformations are available).
\end{itemize}
\end{frame}
%---------------------------------------------------------------------
\begin{frame}{Spatial weights matrix ($\bm{W}$)}
\vspace{-0.5cm}
$$
\bm{C} = \begin{bmatrix}
0 & 1 & 1 & 1 \\
1 & 0 & 1 & 0 \\
1 & 1 & 0 & 1 \\
1 & 0 & 1 & 0
\end{bmatrix}\rightarrow 
\bm{W}=
\begin{bmatrix}
0 & \tfrac{1}{3} & \tfrac{1}{3} & \tfrac{1}{3} \\[2pt]
\tfrac{1}{2} & 0 & \tfrac{1}{2} & 0 \\[2pt]
\tfrac{1}{3} & \tfrac{1}{3} & 0 & \tfrac{1}{3} \\[2pt]
\tfrac{1}{2} & 0 & \tfrac{1}{2} & 0
\end{bmatrix}
$$
\begin{itemize}
	\item Usually, $\bm{W}$ is row-standadrized (to unity): 
	$w_{ij} = \frac{c_{ij}}{\sum^N_{j=1} c_{ij}}$.
	\smallskip
	\item Binary (connectivity) indicators $c_{ij}$ may be generalized prior to standardization. Hence, instead of binary $c_{ij}$, we may use inverse distances (linear, squared, \dots) -- given assumed decay in spatial dependency over distance. \\ \medskip Validity of any such prior information (decay pattern) will influence subsequent analysis (spatial dependency tests, spatial regression models).
\end{itemize}
\end{frame}
%---------------------------------------------------------------------
\begin{frame}{Spatial weights matrix ($\bm{W}$)}
\vspace{-0.2cm}
$$
\bm{W}=
\begin{bmatrix}
0 & \tfrac{1}{3} & \tfrac{1}{3} & \tfrac{1}{3} \\[2pt]
\tfrac{1}{2} & 0 & \tfrac{1}{2} & 0 \\[2pt]
\tfrac{1}{3} & \tfrac{1}{3} & 0 & \tfrac{1}{3} \\[2pt]
\tfrac{1}{2} & 0 & \tfrac{1}{2} & 0
\end{bmatrix}
$$
\begin{itemize}
	\item Each row of $\bm{W}$ ``provides'' weights for an expected value of an observed spatial variable $y_i$ -- weighted averages (fitted values) can be calculated as $\hat{\bm{y}} = \bm{Wy}$. For example:
	\begin{align*}
	\hat{y}_1 & =  \tfrac{1}{3} y_2 + \tfrac{1}{3} y_3 + \tfrac{1}{3} y_4 \\
	&\dots \\
	\hat{y}_4 & = \tfrac{1}{2} y_1  + \tfrac{1}{3} y_3 
	\end{align*}
	\item Observed values of variables can be used to predict corresponding values for neighboring spatial units.
	\smallskip
	\item In this context, $\hat{y}_i$ is often called \textit{spatial lag} of $y_i$.
\end{itemize}
\end{frame}
%---------------------------------------------------------------------
\begin{frame}{Definition of neighbors}
\vspace{-0.3cm}
\begin{figure}
	\includegraphics[width=.7\textwidth]{IMG/sp_MoranPlot.eps}
	\caption[]{Moran plot for unemployment rate, 2014, NUTS2 (AT, CZ, DE, HU, PL, SK): observed values vs. spatial lags.}
\end{figure}
\end{frame}
%---------------------------------------------------------------------
\begin{frame}{Sample selection in spatial data analysis}
\begin{itemize}
    \item Spatially autocorrelated processes are defined in terms of individual units and their interactions with neighbors. 
    \smallskip
    \item Clearly, we can only assess the impact of neighboring units if such units are part of our sample. Hence, in spatial econometrics, we usually do not draw limited samples from a particular area.
    \smallskip
    \item Instead, we work with data from adjacent units located in unbroken (``complete'') study areas.  
    \smallskip
    \item Otherwise, $\bm{C}$ and $\bm{W}$ matrices would be misleading and we could not consistently estimate spatial interactions and effects.
    \smallskip    
    \item Generally speaking, spatial analysis should include the whole geographically defined area/region instead of using random sampling (from a ``population'' of regions within the relevant area). 
\end{itemize}
\end{frame}
%---------------------------------------------------------------------
\subsection{Spatial dependency tests}
\begin{frame}{Spatial dependency tests}
\begin{itemize}
    \item Positive spatial autocorrelation occurs if high or low values \\of a variable cluster in space. 
    \medskip
    \item For negative spatial autocorrelation, spatial units tend to be surrounded by neighbors with very dissimilar observations. 
    \medskip
    \item Spatially random data lack any spatial pattern.
    \medskip
    \item Sometimes, spatial dependency patterns are easy to discern visually using choropleths. 
    \medskip
    \item However, a formal approach towards evaluation of spatial dependency is often required.
\end{itemize}
\end{frame}
%---------------------------------------------------------------------
\begin{frame}{Moran's $I$}
Measure of global spatial autocorrelation, overall clustering of data:
\begin{equation*}
I = \frac{N}{W}\,\bm{z}'\bm{Wz}(\bm{z}'\bm{z})^{-1},
\end{equation*}     
where 
\begin{itemize}
    \item[] $N$ is the number of spatial observations (units) of the variable under scrutiny (say, $\bm{y}$),
    \smallskip
    \item[] $\bm{z}$ is the centered form of $\bm{y}$: $z_i = y_i - \bar{y}$.
    \smallskip
    \item[] The standardization factor $W = \sum_{i}\sum_{j}w_{ij}$ corresponds to the sum of all elements of the spatial weights matrix $\bm{W}$. \\For row-standardized $\bm{W}$ matrices, $\tfrac{N}{W}=1$. Moran's $I$ can be used with $\bm{C}$ matrices (binary, generalized) as well.
\end{itemize}
\end{frame}
%---------------------------------------------------------------------
\begin{frame}{Moran's $I$}
\vspace{-0.2cm}
\begin{equation*}
I = \frac{N}{W}\,\bm{z}'\bm{Wz}(\bm{z}'\bm{z})^{-1},
\end{equation*}     
In most empirical circumstances, $I \in [ -1,1 ]$. Under the null hypothesis of spatial randomness, Moran’s $I$ is asymptotically normally distributed with the following first two moments:
\begin{equation*}
E(I) = -\frac{1}{N-1} \hspace{0.5cm} 
\textnormal{and} \hspace{0.5cm} 
\textnormal{var}(I) = \frac{N^2W_1 - NW_2 +3W^2}{(N^2-1)W^2} \,,
\end{equation*}  
where $W_1 = \sum_i \sum_j (w_{ij}+w_{ji})^2$ and $W_2 = \sum_i (\sum_j w_{ij} + \sum_j w_{ji})^2$.\\
\medskip
Normality assumption $\rightarrow$ calculate a $z$-score 
$$z=\frac{I-E(I)}{\sqrt{\textnormal{var}(I)}} $$\\
and test for statistical significance of Moran's $I$: whether neighboring units are more similar $(I > E(I))$ or more dissimilar $(I < E(I))$ than they would be under the null hypothesis of spatial randomness.
\end{frame}
%---------------------------------------------------------------------




























%---------------------------------------------------------------------
\begin{frame}{Geary's $C$}
Geary's C je definován jako:
$$C = \frac{(N-1)\sum_i \sum_j w_{ij} (X_i - X_j)^2}{2W\sum_i (X_i -\bar{X})^2}$$
kde $N$ je počet prostorových jednotek; $X$ zkoumaná proměnná, $\bar{X}$ je průměrná hodnota, $w_{ij}$ jsou prvky matice vah a $W$ je součet všech prvků $w_{ij}$ . 
\begin{itemize}
	\item Geary's $C$ nabývá hodnotu mezi 0 a 2. 1 představuje prostorovou nezávislost. Hodnoty nižší než 1 znamenají pozitivní prostorovou autokorelaci, narůstající směrem k nule. Hodnoty vyšší než 1, indikují negativní prostorovou autokorelaci.
	\item Geary's $C$ je inverzně spjat se statistikou Moran's $I$. Zatímco Moran's $I$ popisuje celkovou prostorovou autokorelaci, Geary's $C$ je více citlivý na lokální vlivy.
\end{itemize}
\end{frame}
%---------------------------------------------------------------------
\section{Základní modely prostorové regrese}
\begin{frame}{Základní modely prostorové regrese}
\begin{figure}
	\includegraphics[width=.9\textwidth]{IMG/sp_reg.PNG}
	\caption{The rleationship betwee different spatial dependence models for cross-section data (source Halleck Vega and Elhorst 2012)}
\end{figure}
\end{frame}
%---------------------------------------------------------------------
\begin{frame}{Základní modely prostorové regrese}
\begin{itemize}
	\item \textbf{Prostorová autokorelace závislé proměnné}
	$$\bm{y}_t = \rho \bm{W}\bm{y}_t + \dots$$
	$\bm{Wy}_t$ popisuje prostorovou interakci mezi endogenními proměnnými modelu
	$\textit{SpatialLag}(y_{it}) = \bm{w}^T_i \bm{y}_t$, kde je $ \bm{w}^T_i$ řádek matice $\bm{W}$, $\bm{y}_t$ jsou pozorování $y$ v čase $t$.\\
	$\rho$ je koeficient prostorové autokorelace závislé proměnné (skalár); obvykle $\rho \in (-1,1)$.
	\item \textbf{Prostorová autokorelace regresorů}
	$$\bm{y}_t = \dots + \bm{X}_t \bm{\beta} + \bm{W}\bm{X}_t \bm{\theta} + \dots$$
	$\bm{X}$ je obvyklá matice regresorů, $\bm{WX}$ popisuje exogenní prostorové interakce mezi nezávislými proměnnými, $\bm{\beta}$ a $\bm{\theta}$ jsou vektory ($k\times 1$) neznámých parametrů, které se snažíme odhadnout.
\end{itemize}
\end{frame}
%---------------------------------------------------------------------
\begin{frame}{Základní modely prostorové regrese}
\begin{itemize}
	\item \textbf{Prostorová autokorelace náhodné složky} 
	$$\bm{u}_t = \lambda \bm{Wu}_t + \bm{\epsilon}_t$$
	 $\bm{Wu}_t$ popisuje prostorovou interakci mezi náhodnými složkami prostorových jednotek \\
	 $\lambda$ je koeficient prostorové autokorelace náhodné složky, resp. vynechaných prostorově  závislých vysvětlujících proměnných (skalár) \\
	 $\bm{\epsilon}_t$ vektor skutečných náhodných složek – prostorově nezávislá část náhodných složek.
	\item \textbf{Volba modelu se zahrnutím prostorové autokorelace} \\
		Konkrétní typ modelu volíme pomocí omezujících předpokladů: položíme $\rho,\theta$ nebo $\lambda$ rovné nule.
	Omezující předpoklady lze testovat. 
\end{itemize}
\end{frame}
%---------------------------------------------------------------------
\begin{frame}{Základní modely prostorové regrese}
\textbf{Spatial lag model}
\begin{itemize}
	\item Zajímá-li nás prostorová interakce mezi pozorovanými hodnotami závislé proměnné. Prostorovou závislost (strukturu) zde předpokládáme pouze u endogenní proměnné modelu. Model, resp. jeho redukovaná forma, mají tvar: 
	\begin{align*}
	\bm{y}_t & = \rho \bm{Wy}_t + \bm{X}_t\bm{\beta}+ \bm{u}_t\\
	(\bm{I} - \rho \bm{W}) \bm{y}_t & = \bm{X}_t \bm{\beta} + \bm{u}_t
	\end{align*}
	\item Koeficienty $\rho$ i $\bm{\beta}$ jsou odhadnuty metodou maximální věrohodnosti (MLE). Koeficienty  $\bm{\beta}$  pomáhají vysvětlit tu část variability $\bm{y}$, která není vysvětlena prostorově.	
\end{itemize}
\end{frame}
%---------------------------------------------------------------------
\begin{frame}{Základní modely prostorové regrese}
\textbf{Spatial Durbin model}
\begin{itemize}
	\item Vznikne rozšířením specifikace Spatial lag model o prostorové interakce mezi regresory: 
		\begin{align*}
		\bm{y}_t & = \rho \bm{Wy}_t + \bm{X}_t\bm{\beta}+ \bm{Wy}_t\bm{X}_t\bm{\theta} +  \bm{u}_t\\
		(\bm{I} - \rho \bm{W}) \bm{y}_t & = \bm{X}_t \bm{\beta} + \bm{Wy}_t\bm{X}_t\bm{\theta}+ \bm{u}_t
		\end{align*}
\end{itemize}
\end{frame}
%---------------------------------------------------------------------
\begin{frame}{Základní modely prostorové regrese}
\textbf{Spatial error model}
	\begin{align*}
	\bm{y}_t & =   \bm{X}_t\bm{\beta}+  \bm{u}_t, \qquad \bm{u}_t = \lambda  \bm{Wu}_t + \bm{\epsilon}_t\\
	\bm{y}_t & = \bm{X}_t\bm{\beta}+ \lambda\bm{Wy}_t\bm{u}_t+  \bm{\epsilon}_t\\
	(\bm{I} - \lambda \bm{W}) \bm{y}_t & = (\bm{I} - \lambda \bm{W}) \bm{X}_t \bm{\beta} + \bm{\epsilon}_t
	\end{align*}
\begin{itemize}

	\item I v případě, kdy se nezajímáme o prostorové interakce mezi pozorovanými proměnnými, můžeme zlepšit vlastnosti odhadu pomocí prostorově závislých chyb.
	\item Interakce v rámci náhodné složky nevyžadují teoretické zdůvodnění interakčního procesu, předpokládáme existenci prostorově závislých proměnných, nezahrnutých do modelu
\end{itemize}
\end{frame}
%---------------------------------------------------------------------
\begin{frame}{Prostorové modely:  stacionarita/stabilita}
\textbf{Stacionarita}
\begin{itemize}
\item Neformálně: pro každou oblast je počet sousedících oblastí omezený (malý)
\item Řádkové (a sloupcové) součty matice $\bm{S}$ jsou konečné a omezené, i když počet uvažovaných prostorových jednotek $n$ roste neomezeně.
\item Korelace mezi dvěma prostorovými jednotkami konverguje k nule s rostoucí vzdáleností mezi těmito jednotkami.
\item Formální podmínky stacionarity (pro $\rho, \lambda$ a $\bm{W}$ ) viz \href{https://www.google.cz/url?sa=t&rct=j&q=&esrc=s&source=web&cd=1&cad=rja&uact=8&ved=0ahUKEwjjwvLCk8bLAhXEvXIKHWSHBxsQFgggMAA&url=http://www.springer.com/cda/content/document/cda_downloaddocument/9783642403392-c2.pdf?SGWID\%3D0-0-45-1432965-p175381976&usg=AFQjCNGpme8ofJQzd46BCJVsUEio6oKIzQ&sig2=LlarV9wYhELGcTPiqzfRgg}{Elhorst (2014)}.
\end{itemize}
\end{frame}
%---------------------------------------------------------------------
\begin{frame}{Prostorové modely:  stacionarita/stabilita}
\textbf{Stabilita odhadů}
\begin{itemize}
	\item Matice $\bm{S}$, resp. matice $\bm{W}$ není odhadována, musíme ji stanovit předem. Odhadnuté koeficienty $\bm{\beta}$, resp. $\bm{\theta}$ závisejí na zvolené matici $\bm{W}$.
	\item "Řešení": regresní modely odhadujeme opakovaně, pokaždé s trochu jinak definovanou maticí $\bm{W}$ a porovnáváme vlastnosti jednotlivých odhadů.
    \item Tento postup zaručuje pouze lokální optimum (určíme nejlepší z porovnávaných modelů podle zvoleného kritéria), nikoliv globální optimum.
	\end{itemize}
\end{frame}
%---------------------------------------------------------------------
\section{Přímé a nepřímé efekty (Spill-overs)}
\begin{frame}{Přímé a nepřímé efekty (Spill-overs)}
\begin{itemize}
	\item $(\bm{I} - \rho \bm{W}) \bm{y}_t = \bm{X}_t \bm{\beta} + \bm{W}\bm{X}_t\bm{\theta} + \alpha \bm{\iota} +\bm{u}_t$ \quad $ \alpha \bm{\iota}$ je vektor úroňových konstant
	\item $\bm{y}_t = (\bm{I} - \rho \bm{W})^{-1} (\bm{X}_t\bm{\beta} + \bm{W}\bm{X}_t \bm{\theta})+ \bm{R}_t$ \quad 
	$\bm{R}_t$ obsahuje náhodnou složku i intercept 
\end{itemize}
Derivaci $E(\bm{y}_t)$ podle $k$-té vysvětlující proměnné $\bm{x}_{kt}$ lze zapsat jako:\\(časový index $t$ pro jednuduchost vynecháme)

\begin{align*} \bigg[\frac{\partial E(\bm{y})}{\partial \bm{x}_{1,k}} \dots \frac{\partial E(\bm{y})}{\partial \bm{x}_{N,k}}\bigg] & =
\begin{bmatrix}
\frac{\partial E(y_1)}{\partial \bm{x}_{1,k}} & \dots & \frac{\partial E(y_1)}{\partial \bm{x}_{N,k}} \\
\vdots & \ddots & \vdots \\
\frac{\partial E(y_N)}{\partial \bm{x}_{1,k}} & \dots & \frac{\partial E(y_N)}{\partial \bm{x}_{N,k}}
\end{bmatrix} = \\
& = (\bm{I} - \rho \bm{W})^{-1} 
\begin{bmatrix}
\beta_k & w_{12}\theta_k & \dots & w_{1N}\theta_k \\
w_{21}\theta_K & \beta_k & \dots & w_{2N}\theta_k \\
\vdots & \ddots & \ddots & \vdots \\
w_{N1}\theta_k & w_{N2}\theta_k & \dots & \beta_k 
\end{bmatrix}
\end{align*}
\end{frame}
%---------------------------------------------------------------------
\begin{frame}{Přímé a nepřímé efekty (Spill-overs)}
$$ \bigg[\frac{\partial E(\bm{y})}{\partial \bm{x}_{1,k}} \dots \frac{\partial E(\bm{y})}{\partial \bm{x}_{N,k}}\bigg] =
 (\bm{I} - \rho \bm{W})^{-1} 
\begin{bmatrix}
\beta_k & w_{12}\theta_k & \dots & w_{1N}\theta_k \\
w_{21}\theta_K & \beta_k & \dots & w_{2N}\theta_k \\
\vdots & \ddots & \ddots & \vdots \\
w_{N1}\theta_k & w_{N2}\theta_k & \dots & \beta_k 
\end{bmatrix}
$$
%---------------------------------------------------------------------
\begin{itemize}
	\item Změním-li hodnotu vysvětlující proměnné $x_k$ u $i$-té prostorové jednotky (změna $x_{ik}$), dojde ke změně očekávané hodnoty $y_i$ - \textbf{přímý efekt} a zároveň dojde ke změně očekávané hodnoty $y$ u dalších prostorových jednotek – \textbf{nepřímý efekt}. 
	\item Diagonální prvky matice (RHS) představují přímé efekty, prvky mimo diagonálu jsou nepřímé efekty
\end{itemize}

\end{frame}
%---------------------------------------------------------------------
\begin{frame}{Přímé a nepřímé efekty (Spill-overs)}
$$ \bigg[\frac{\partial E(\bm{y})}{\partial \bm{x}_{1,k}} \dots \frac{\partial E(\bm{y})}{\partial \bm{x}_{N,k}}\bigg] =
(\bm{I} - \rho \bm{W})^{-1} 
\begin{bmatrix}
\beta_k & w_{12}\theta_k & \dots & w_{1N}\theta_k \\
w_{21}\theta_K & \beta_k & \dots & w_{2N}\theta_k \\
\vdots & \ddots & \ddots & \vdots \\
w_{N1}\theta_k & w_{N2}\theta_k & \dots & \beta_k 
\end{bmatrix}
$$

\begin{itemize}
	\item Přímé a nepřímé efekty se liší pro jednotlivé prostorové jednotky ($i$).
	\begin{itemize}
	\item 	Přímé efekty se liší, protože diagonální prvky $(\bm{I} - \rho \bm{W})^{-1} $ jsou různé, pokud $\rho \neq 0$.
	\item Nepřímé efekty se liší, protože prvky mimo diagonálu matice $\bm{W}$, resp. $(\bm{I} - \rho \bm{W})^{-1}$ jsou různé, pokud $\rho \neq 0$ a/nebo $\theta_k \neq 0$
\end{itemize}
	$$ (\bm{I} - \rho \bm{W})^{-1} = \bm{I} + \rho\bm{W} + \rho^2\bm{W}^2 + \rho^3\bm{W}^3 + \dots$$

	\item Diagonální prvky matice (RHS) představují přímé efekty, prvky mimo diagonálu jsou nepřímé efekty
\end{itemize}
\end{frame}
%---------------------------------------------------------------------
\begin{frame}{Přímé a nepřímé efekty (Spill-overs)}
$$ \bigg[\frac{\partial E(\bm{y})}{\partial \bm{x}_{1,k}} \dots \frac{\partial E(\bm{y})}{\partial \bm{x}_{N,k}}\bigg] =
(\bm{I} - \rho \bm{W})^{-1} 
\begin{bmatrix}
\beta_k & w_{12}\theta_k & \dots & w_{1N}\theta_k \\
w_{21}\theta_K & \beta_k & \dots & w_{2N}\theta_k \\
\vdots & \ddots & \ddots & \vdots \\
w_{N1}\theta_k & w_{N2}\theta_k & \dots & \beta_k 
\end{bmatrix}
$$
%---------------------------------------------------------------------
\begin{itemize}
\item \textbf{Interpretace odhadnutého prostorového regresního modelu – vždy na základě přímých a nepřímých efektů}. 
\item Koeficient $\beta_k$ nepopisuje chování závislé proměnné $y_i$ při změně $x_{ik}$ "správně".
\item Statistickou významnost přímých efektů můžeme testovat viz \href{https://www.google.cz/url?sa=t&rct=j&q=&esrc=s&source=web&cd=1&cad=rja&uact=8&ved=0ahUKEwjjwvLCk8bLAhXEvXIKHWSHBxsQFgggMAA&url=http://www.springer.com/cda/content/document/cda_downloaddocument/9783642403392-c2.pdf?SGWID\%3D0-0-45-1432965-p175381976&usg=AFQjCNGpme8ofJQzd46BCJVsUEio6oKIzQ&sig2=LlarV9wYhELGcTPiqzfRgg}{Elhorst (2014)}.	
\end{itemize}
\end{frame}
%---------------------------------------------------------------------
\begin{frame}{Přímé a nepřímé efekty (Spill-overs)}
\begin{itemize}
	\item Ukázkový výstup odhadu přímých a nepřímých efektů (LeSage/Pace, 2009):
	$$\bm{y} = (\bm{I} - \rho \bm{W})^{-1} \bm{X}\bm{\beta} + (\bm{I} - \rho \bm{W})^{-1} \bm{\epsilon}$$
	$\dots$ jednoduchá regrese $y$ na $x$, použita prostorová regrese: \\
	Spatial lag model.
	
	\end{itemize}
\begin{table}[]
\centering
\caption{Spatial partitioning of direct, indirect and total   impacts}
\begin{tabular}{llll}
&         &          &             \\
                                                                                                       & Mean    & Std. dev & t-statistic \\
Direct effect                                                                                          & 0.586   & 0.0148   & 39.6106     \\
Indirect effect                                                                                        & 1.08414 & 0.0587   & 18.4745     \\
Total effect                                                                                           & 1.67    & 0.0735   & 22.7302    
\end{tabular}
\end{table}
\end{frame}
%---------------------------------------------------------------------
\begin{frame}{Přímé a nepřímé efekty (Spill-overs)}

$$ \bigg[\frac{\partial E(\bm{y})}{\partial \bm{x}_{1,k}} \dots \frac{\partial E(\bm{y})}{\partial \bm{x}_{N,k}}\bigg] =
(\bm{I} - \rho \bm{W})^{-1} 
\begin{bmatrix}
\beta_k & w_{12}\theta_k & \dots & w_{1N}\theta_k \\
w_{21}\theta_K & \beta_k & \dots & w_{2N}\theta_k \\
\vdots & \ddots & \ddots & \vdots \\
w_{N1}\theta_k & w_{N2}\theta_k & \dots & \beta_k 
\end{bmatrix}
$$
\begin{itemize}
	\item \textbf{Nevýhody, omezení modelů založených na prostorové závislosti:}
	\item Poměr přímého a nepřímého efektu nezávisí na $\beta_k$ - je dán pouze na základě matice $\bm{W}$ a hodnoty $\rho$. 
	\item Tento poměr je stejný pro všechny vysvětlující proměnné (resp. příslušné přímé a nepřímé efekty).
\end{itemize}

\end{frame}
%---------------------------------------------------------------------
\section{Identifikace hotspotů a coldspotů}
\begin{frame}{Identifikace "hotspotů" a "coldspotů"}
\begin{itemize}
	\item \textbf{Getis-Ord $G^\ast_i$}
	$$ G^\ast_i = \frac{\sum_{j=1}^n w_{i,j} x_j - \bar{X} \sum_{j=1}^n w_{i,j}}{S \quad\sqrt[]{\frac{[n\sum_{j=1}^n w^2_{i,j}-(\sum_{j=1}^n w_{i,j})^2]}{n-1}}}$$
	$$ \bar{X} = \frac{\sum_{j=1}^n x_j}{n}, \quad S = \sqrt[]{\frac{\sum_{j=1}^n x^2_j}{n}-(\bar{X})^2} $$
	\item $x_j$ je hodnota sledované proměnné v $j$-té jednotce, $w_{i,j}$ jsou prvky matice $\bm{W}$,  $n$ je počet prostorových jednotek.
	\item $G^\ast_i$ statistika se používá pouze pro kladnou prostorovou autokorelaci a umožňuje identifikovat prostorové shluky jednotek, které mají podobné hodnoty, jež jsou vysoké nebo nízké vůči průměrné pozorované hodnotě – hotspoty a coldspoty.\\(Getis – Ord 1992).
\end{itemize}
\end{frame}
%---------------------------------------------------------------------
\begin{frame}{Identifikace "hotspotů" a "coldspotů"}
\begin{figure}
	\includegraphics[width=.7\textwidth]{IMG/sp_coldspot.PNG}
	\caption{Spatial clusters (hot and cold spots) of the municipalities in Serbia by the level of average monthly net earning from 2001 to 2010 (at fix distance of 210 km)}
\end{figure}
\end{frame}
%---------------------------------------------------------------------
\begin{frame}{Identifikace "hotspotů" a "coldspotů"}
	$$ G^\ast_i = \frac{\sum_{j=1}^n w_{i,j} x_j - \bar{X} \sum_{j=1}^n w_{i,j}}{S \quad \sqrt[]{\frac{[n\sum_{j=1}^n w^2_{i,j}-(\sum_{j=1}^n w_{i,j})^2]}{n-1}}}$$
\begin{itemize}
	\item $ G^\ast_i$ je lokální statistika – spočtená pro každou prostorovou jednotku, jedná se o tzv. $z$-poměr ($z$-score).
	\item $ G^\ast_i$ popisuje statistickou významnost prostorového shlukování. Vysoká pozitivní hodnota $G^\ast_i$ indikuje koncentraci vysokých hodnot u sousedících prostorových jednotek (hotspot). Naopak, negativní hodnoty indikují výskyt tzv. coldspotu.
	\item Hodnoty blízké nule naznačují, že jednotka není součástí žádného prostorového shluku.
	
\end{itemize}
\end{frame}
%---------------------------------------------------------------------
\section{Prostorová regrese v R}
\begin{frame}{Prostorová regrese v R}
\begin{itemize}
	\item Knihovna \{spdep\} 
	\begin{itemize}
		\item Různé typy prostorových modelů
		\item Testy prostorové nezávislosti, identifikace prostorových shluků,$\dots$
	\end{itemize}
	\item	Knihovna \{splm\}
	\begin{itemize}
		\item Prostorová regrese na panelových datech
	\end{itemize}
	\item Knihovna \{ggplot2\} + další nutné knihovny
	\begin{itemize}
		\item Kartogramy (infomapy) / choropleth maps
	\end{itemize}
\end{itemize}
 \begin{figure}
 	\includegraphics[width=.5\textwidth]{IMG/sp_unemp.PNG}
 \end{figure}

\end{frame}

%---------------------------------------------------------------------


\end{document}